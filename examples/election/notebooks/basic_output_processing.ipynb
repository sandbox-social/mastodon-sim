{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4890b769-a2a0-4bdf-87c9-ca1c4ea9b2c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/mnt/c/Users/maxpu/Dropbox/scripts/Projects/socialsandbox/mastodon-sim\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "import json\n",
    "import os\n",
    "import yaml\n",
    "current_path = os.getcwd()\n",
    "print(current_path)\n",
    "assert current_path.split('/')[-1]==\"mastodon-sim\", \"run the notebook from project root!\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92a088eb-d779-4d62-9507-a62c8ef35b82",
   "metadata": {},
   "source": [
    "load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1d0663e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "project root: /mnt/c/Users/maxpu/Dropbox/scripts/Projects/socialsandbox/mastodon-sim\n",
      "/mnt/c/Users/maxpu/Dropbox/scripts/Projects/socialsandbox/mastodon-sim/examples/election/outputs\n"
     ]
    }
   ],
   "source": [
    "PROJECT_ROOT = current_path\n",
    "print(\"project root: \" + str(PROJECT_ROOT))\n",
    "abs_rootpath = PROJECT_ROOT + \"/examples/election/outputs\"\n",
    "print(abs_rootpath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "815a236e-50ce-4df8-80b8-5c48c7f8965d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# in output_proc_utils.py\n",
    "\n",
    "import collections\n",
    "\n",
    "def load_data(fileroot, names_of_focalplayers):\n",
    "    with open(fileroot[0]+'/'+fileroot[1] +\"/.hydra/config.yaml\") as file:\n",
    "        config_data = yaml.safe_load(file)\n",
    "    outfile_path = fileroot[0]+'/'+fileroot[1]+'/'+fileroot[1]+'_'+fileroot[2] + \"_events.jsonl\"\n",
    "    print(outfile_path)\n",
    "    df = pd.read_json(outfile_path, lines=True)\n",
    "\n",
    "    pd.set_option(\"display.width\", 1000)\n",
    "    print(df.head())\n",
    "    print()\n",
    "    print(\"Probes:\")\n",
    "    print(df.loc[df.event_type == \"probe\", \"label\"].value_counts())\n",
    "    print()\n",
    "    print(\"Actions:\")\n",
    "    print(df.loc[(df.event_type == \"action\") & (df.episode > -1), \"label\"].value_counts())\n",
    "    print()\n",
    "    print(\n",
    "        df.loc[(df.event_type == \"action\") & (df.episode > -1) & df.data.apply(lambda x: type(x)==dict), \"data\"]\n",
    "        .apply(lambda x: x.keys())\n",
    "        .value_counts()\n",
    "    )\n",
    "    print()\n",
    "    print(\n",
    "        df.loc[\n",
    "            (df.source_user.isin(names_of_focalplayers))\n",
    "            & (df.event_type == \"action\")\n",
    "            & (df.episode > -1)\n",
    "        ]\n",
    "        .groupby(\"source_user\")[\"label\"]\n",
    "        .value_counts()\n",
    "    )\n",
    "    print()\n",
    "    print(\"post\")\n",
    "    posts = df.loc[\n",
    "        (df.label == \"post\") & (df.episode > -1) & (df.source_user != \"storhampton_gazette\"),\n",
    "        [\"source_user\", \"data\"],\n",
    "    ]\n",
    "    users = posts[\"source_user\"].values\n",
    "    posts = posts[\"data\"].apply(lambda x: x[\"post_text\"]).values\n",
    "    for user, post in zip(users, posts):\n",
    "        print(f\"source_user:{user}:{post}\")\n",
    "    print()\n",
    "    print(\"reply\")\n",
    "    replies = df.loc[\n",
    "        (df.label == \"reply\") & (df.episode > -1) & (df.source_user != \"storhampton_gazette\"),\n",
    "        [\"source_user\", \"episode\", \"data\"],\n",
    "    ]\n",
    "    users = replies[\"source_user\"].values\n",
    "    replies = replies[\"data\"].apply(lambda x: x[\"post_text\"]).values\n",
    "    for user, reply in zip(users, replies):\n",
    "        print(f\"source_user:{user}:{reply}\")\n",
    "    print()\n",
    "    dftmp = df.copy()\n",
    "    dftmp = dftmp.loc[(dftmp.event_type == \"action\") & (dftmp.episode > -1), :]\n",
    "    dftmp[\"data\"] = dftmp[\"data\"].apply(str)\n",
    "    print(f\"{(len(dftmp) - len(dftmp.drop_duplicates())) / len(dftmp):.3f} duplicate fraction\")\n",
    "    return df\n",
    "\n",
    "\n",
    "def post_process_output(df):\n",
    "    eval_df = df.loc[\n",
    "        df.event_type == \"eval\", [\"episode\", \"source_user\", \"label\", \"data\"]\n",
    "    ].reset_index(drop=True)\n",
    "    eval_df[\"response\"] = eval_df.data.apply(lambda x: x[\"query_return\"])\n",
    "    eval_df = eval_df.drop(\"data\", axis=1)\n",
    "\n",
    "    edge_df = df.loc[\n",
    "        df.label.isin([\"follow\", \"unfollow\"]), [\"episode\", \"source_user\", \"data\", \"label\"]\n",
    "    ].reset_index(drop=True)\n",
    "    edge_df[\"target_user\"] = edge_df.data.apply(lambda d: d[\"target_user\"])\n",
    "    edge_df = edge_df.drop(\"data\", axis=1)\n",
    "\n",
    "    interaction_types = [\"post\", \"like_toot\", \"boost_toot\", \"reply\"]\n",
    "    int_df = df.loc[df.label.isin(interaction_types), :].reset_index(drop=True)\n",
    "    return eval_df, int_df, edge_df\n",
    "\n",
    "\n",
    "def episodewise_graphbuild(edge_df):\n",
    "    follow_graph = nx.DiGraph()\n",
    "    for epi_edge_data in edge_df.groupby(\"episode\"):\n",
    "        for action, operate_on_graph in zip(\n",
    "            [\"follow\", \"unfollow\"], [follow_graph.add_edges_from, follow_graph.remove_edges_from]\n",
    "        ):\n",
    "            if (epi_edge_data.label == action).any():\n",
    "                data = epi_edge_data.loc[\n",
    "                    epi_edge_data.label == action, [\"source_user\", \"target_user\"]\n",
    "                ]\n",
    "                operate_on_graph(list(data.itertuples(index=False, name=None)))\n",
    "    return follow_graph\n",
    "\n",
    "\n",
    "# in dashboard-basic.py:\n",
    "\n",
    "\n",
    "def get_target_user(row):\n",
    "    if row.label == \"post\":\n",
    "        target_user = row.source_user\n",
    "    elif row.label == \"like_toot\":\n",
    "        target_user = row.data[\"target_user\"]\n",
    "    elif row.label == \"boost_toot\":\n",
    "        target_user = row.data[\"target_user\"]\n",
    "    elif row.label == \"reply\":\n",
    "        target_user = row.data[\"reply_to\"][\"target_user\"]\n",
    "    return target_user\n",
    "\n",
    "\n",
    "def get_int_dict(int_df):\n",
    "    past = dict(\n",
    "        zip([\"post\", \"like_toot\", \"boost_toot\", \"reply\"], [\"posted\", \"liked\", \"boosted\", \"replied\"])\n",
    "    )\n",
    "    int_df[\"int_data\"] = int_df.apply(\n",
    "        lambda x: {\n",
    "            \"action\": past[x.label],\n",
    "            \"episode\": x.episode,\n",
    "            \"source_user\": x.source_user,\n",
    "            \"target_user\": get_target_user(x),\n",
    "            \"toot_id\": x.data[\"toot_id\"],\n",
    "        },\n",
    "        axis=1,\n",
    "    )\n",
    "    int_df.int_data = int_df.apply(\n",
    "        lambda x: x.int_data | {\"parent_toot_id\": x.data[\"reply_to\"][\"toot_id\"]}\n",
    "        if x.label == \"reply\"\n",
    "        else x.int_data,\n",
    "        axis=1,\n",
    "    )\n",
    "    return int_df.groupby(\"episode\")[\"int_data\"].apply(list).to_dict()\n",
    "\n",
    "\n",
    "def get_toot_dict(int_df):\n",
    "    past = dict(\n",
    "        zip([\"post\", \"like_toot\", \"boost_toot\", \"reply\"], [\"posted\", \"liked\", \"boosted\", \"replied\"])\n",
    "    )\n",
    "    text_df = int_df.loc[(int_df.label == \"post\") | (int_df.label == \"reply\"), :].reset_index(\n",
    "        drop=True\n",
    "    )\n",
    "\n",
    "    # handle Nones as toot_ids by appending an index\n",
    "    no_toot_id = text_df.data.apply(lambda x: x[\"toot_id\"] is None)\n",
    "    text_df[\"no_toot_id_idx\"] = -1\n",
    "    text_df.loc[no_toot_id, \"no_toot_id_idx\"] = range(no_toot_id.sum())\n",
    "    text_df.loc[no_toot_id, \"data\"] = text_df.loc[no_toot_id, :].apply(\n",
    "        lambda x: x.data | {\"toot_id\": \"None\" + str(x.no_toot_id_idx)}, axis=1\n",
    "    )\n",
    "\n",
    "    text_df[\"toot_id\"] = text_df.data.apply(lambda x: x[\"toot_id\"])\n",
    "    text_df = text_df.set_index(\"toot_id\")\n",
    "    text_df[\"text_data\"] = text_df.apply(\n",
    "        lambda x: {\"user\": x.source_user, \"action\": past[x.label], \"content\": x.data[\"post_text\"]},\n",
    "        axis=1,\n",
    "    )\n",
    "    text_df.text_data = text_df.apply(\n",
    "        lambda x: x.text_data | {\"parent_toot_id\": x.data[\"reply_to\"][\"toot_id\"]}\n",
    "        if x.label == \"reply\"\n",
    "        else x.text_data,\n",
    "        axis=1,\n",
    "    )\n",
    "\n",
    "    return text_df.text_data.to_dict()\n",
    "\n",
    "\n",
    "def load_data_dash(eval_df, int_df, edge_df):\n",
    "    # votes\n",
    "    votes = (\n",
    "        eval_df.loc[eval_df.label == \"vote_pref\", [\"source_user\", \"response\", \"episode\"]]\n",
    "        .groupby(\"episode\")\n",
    "        .apply(lambda x: dict(zip(x.source_user, x.response)))\n",
    "        .to_dict()\n",
    "    )\n",
    "\n",
    "    # final follow network\n",
    "    follow_graph = nx.from_pandas_edgelist(\n",
    "        edge_df, \"source_user\", \"target_user\", create_using=nx.DiGraph()\n",
    "    )  # invalid in presence of unfollows (in which case use episodewise_graphbuild)\n",
    "\n",
    "    # active users with episode keys\n",
    "    posted_users_by_episode = int_df.groupby(\"episode\")[\"source_user\"].apply(set).to_dict()\n",
    "\n",
    "    # interaction data\n",
    "    int_dict = get_int_dict(int_df.copy())\n",
    "\n",
    "    # toot_data\n",
    "    toot_dict = get_toot_dict(int_df.copy())\n",
    "\n",
    "    return follow_graph, int_dict, posted_users_by_episode, toot_dict, votes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c6487ac6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/mnt/c/Users/maxpu/Dropbox/scripts/Projects/socialsandbox/mastodon-sim/examples/election/outputs/N20_T1_Reddit.Big5_independent_v1_news_no_bias_with_images_run1/N20_T1_Reddit.Big5_independent_v1_news_no_bias_with_images_run1_2025-05-01_22-13-51_events.jsonl\n",
      "      source_user   label                                               data  episode event_type\n",
      "0   Eric Thompson  follow  {'target_user': 'Storhampton Gazette', 'sugges...       -1     action\n",
      "1  Luciana Santos  follow  {'target_user': 'Alex Carter', 'suggested_acti...       -1     action\n",
      "2  Sophia Johnson  follow  {'target_user': 'Bradley Carter', 'suggested_a...       -1     action\n",
      "3  Nathan Ramirez  follow  {'target_user': 'Bradley Carter', 'suggested_a...       -1     action\n",
      "4      Emily Chen  follow  {'target_user': 'Eric Thompson', 'suggested_ac...       -1     action\n",
      "\n",
      "Probes:\n",
      "label\n",
      "Favorability    40\n",
      "VotePref        20\n",
      "VoteIntent      20\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Actions:\n",
      "label\n",
      "inner_actions    57\n",
      "reply            27\n",
      "episode_plan     18\n",
      "like_toot         4\n",
      "boost_toot        2\n",
      "post              2\n",
      "Name: count, dtype: int64\n",
      "\n",
      "data\n",
      "(reply_to, toot_id, post_text, suggested_action)    27\n",
      "(toot_id, target_user, suggested_action)             6\n",
      "(toot_id, post_text, suggested_action)               2\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Series([], Name: count, dtype: int64)\n",
      "\n",
      "post\n",
      "source_user:Emily Chen:As we approach the election, I can’t help but think about the long-term effects of our choices on education and community well-being. How do we ensure that our voices are heard in this critical time? Let's discuss the implications of the candidates' policies! 🌱 #StorhamptonElection #EducationMatters\n",
      "source_user:Storhampton Gazette:Among Fredrickson’s Day One Initiatives: Canceling Carter's Industrial Regulation Policy\n",
      "\n",
      "reply\n",
      "source_user:Emily Chen:Thank you for highlighting this, Luciana! It's essential that we advocate for policies that truly serve our community's needs, especially in education. Together, we can ensure that every voice is heard! 🌟 #CommunityFirst\n",
      "source_user:Zoe Lin:Thank you for highlighting the importance of considering marginalized communities, Rachel! It's crucial that we advocate for policies that uplift everyone as we prepare to vote. How do you think we can ensure all voices are heard in this election? 💚🗳️\n",
      "source_user:Rachel Thompson:Absolutely, Lisa! It's essential we choose candidates who genuinely prioritize our community's needs over corporate interests. Together, we can advocate for policies that uplift everyone! 💪 #VoteForChange\n",
      "source_user:Arjun Patel:Absolutely, Michael! Bill Fredrickson's focus on job creation is exactly what Storhampton needs right now. Let's rally behind policies that drive our economy forward! 💼 #VoteForGrowth\n",
      "source_user:Elijah Martin:Absolutely, Luciana! It's crucial that we prioritize the voices of our community, especially in this election. We need leaders who truly represent us, not just corporate interests. #VoteSmart #CommunityFirst\n",
      "source_user:Michael Thompson:Absolutely, Luciana! It's crucial that we advocate for candidates who genuinely prioritize our community's needs. Let's ensure our voices are heard in this election! #CommunityFirst\n",
      "source_user:Alex Carter:Absolutely, Luciana! It's vital we choose candidates who genuinely prioritize our community's needs over corporate interests. What policies do you think are most crucial for us right now? 🤔 #VoteSmart\n",
      "source_user:David Nguyen:I completely agree, Luciana! It's vital that we prioritize our community's needs in this election. I'm hopeful that candidates who genuinely care about us will emerge victorious. What specific policies do you think we should advocate for? 🤔 #CommunityFirst\n",
      "source_user:Nathan Ramirez:I think balancing economic growth with environmental protection is crucial. We need to ensure that our community thrives while also being responsible stewards of our environment. Let's keep the conversation going! 🌱💼 #StorhamptonElection\n",
      "source_user:Michael Donovan:I agree that community needs are crucial, Luciana! However, we must also ensure that our economic growth isn't stifled by regulations. A balanced approach will empower us all! 💼🌍 #StorhamptonElection\n",
      "source_user:Alex Carter:Great points, Michael! It's crucial we hold our candidates accountable. How do you think we can ensure that economic growth doesn't come at the expense of our community's well-being? 🤔 #BalanceIsKey\n",
      "source_user:Nathan Ramirez:I appreciate your perspective, Rachel! It's vital that we consider the impact of our choices on all community members as we move forward. Let's strive for a balance that promotes both growth and equity. 💚 #StorhamptonElection\n",
      "source_user:Jessica Lopez:I completely agree, Eric! It's vital that we find innovative solutions that prioritize both economic growth and social equity. How do you think we can ensure that all voices are included in this conversation? #StorhamptonElection\n",
      "source_user:Michael Donovan:I completely agree, Alex! It's vital that we hold our candidates accountable and ensure that economic growth aligns with our community's well-being. Let's keep pushing for that balance! 💼🌍 #BalanceIsKey\n",
      "source_user:Sophia Johnson:Absolutely, Rachel! It’s essential to uplift marginalized voices as we make our choices. Let's advocate for compassion and justice in this election! 💚 #SocialEquity\n",
      "source_user:Nathan Ramirez:I completely agree, Javier! Supporting policies that foster economic growth and job creation is essential for our community's future. Let's rally behind Bill Fredrickson and make our voices heard!\n",
      "source_user:Eric Thompson:Absolutely, Emily! Education is foundational to our community's future. How can we ensure our voices are prioritized in this election? Let's brainstorm some ideas together! 🌱 #EducationMatters\n",
      "source_user:Luciana Santos:Absolutely, Nathan! Striking that balance is essential. We must advocate for policies that protect our environment while promoting economic growth. What specific measures do you think we should push for? 🌱💬 #Sustainability\n",
      "source_user:Javier Gonzalez:Great points, Alex! I believe we must ensure that economic growth supports our community's well-being. What specific policies do you think could achieve this balance? 🤔 #CommunityFirst\n",
      "source_user:Jessica Lopez:I appreciate you bringing attention to the importance of uplifting marginalized communities as we approach the election. It's essential that we ensure all voices are heard in this process! 💚🗳️\n",
      "source_user:Michael Donovan:Great point, Zoe! While we must protect our environment, we cannot overlook the economic implications of strict regulations. A balanced approach is essential for a thriving community! 🌍💼 #StorhamptonElection\n",
      "source_user:Eric Thompson:While I see the potential in Bill Fredrickson's job creation plans, I'm curious about how these tax breaks will directly benefit our community's most vulnerable members. What are your thoughts? 🤔 #CommunityFirst\n",
      "source_user:Zoe Lin:Great point, Tom! Balancing economic growth with environmental protection is vital. What strategies do you think could effectively achieve this balance? 🌱💡\n",
      "source_user:Tom Richards:Great points, Emily! It's crucial that we consider how each candidate's policies will shape our community's future. I believe we need to prioritize environmental protection while also supporting education. What are your thoughts on Carter's approach to sustainability?\n",
      "source_user:Tom Richards:I completely agree, Zoe! Finding that balance between economic growth and environmental protection is essential. What strategies do you think we can implement to make this happen?\n",
      "source_user:Tom Richards:I completely agree, Emily! It's crucial that we discuss the long-term effects of our choices on education and community well-being. How can we ensure that our voices are prioritized in this election?\n",
      "source_user:Tom Richards:I love your enthusiasm, Sophia! It's so important for us to come together and make a positive impact on our community and the environment. Let's keep pushing for a greener future!\n",
      "\n",
      "0.009 duplicate fraction\n"
     ]
    }
   ],
   "source": [
    "# run_name = [\"N20_T8_Reddit.Big5_independent_v1_news_no_bias_with_images_run1\",\"2025-03-22_02-26-35\"]\n",
    "# run_name = [\"N20_T4_Reddit.Big5_independent_v1_news_no_bias_with_images_run1\",\"2025-04-09_23-34-04\"]\n",
    "# run_name = [\"N20_T1_Reddit.Big5_independent_v1_news_no_bias_with_images_run1\",\"2025-05-01_21-44-23\"]\n",
    "run_name = [\"N20_T1_Reddit.Big5_independent_v1_news_no_bias_with_images_run1\",\"2025-05-01_22-13-51\"]\n",
    "\n",
    "\n",
    "\n",
    "names_of_focalplayers = [\"Bill Fredrickson\", \"Bradley Carter\"]\n",
    "df = load_data([abs_rootpath] + run_name, names_of_focalplayers)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c74d2ed9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6485623003194888"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df.label=='inner_actions'].groupby(['source_user', 'episode'])['data'].apply(lambda x: x.str.count(\"SUCCESS\")).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f4c539f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>episode</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>source_user</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Alex Thompson</th>\n",
       "      <td>110</td>\n",
       "      <td>110</td>\n",
       "      <td>NaN</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Bill Fredrickson</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Bradley Carter</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>110010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Carlos Mendes</th>\n",
       "      <td>11000</td>\n",
       "      <td>11</td>\n",
       "      <td>1100000001</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Chris Anderson</th>\n",
       "      <td>1111111</td>\n",
       "      <td>1101111111011</td>\n",
       "      <td>1110101</td>\n",
       "      <td>11001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Emily Chen</th>\n",
       "      <td>11</td>\n",
       "      <td>11</td>\n",
       "      <td>11</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Emily Thompson</th>\n",
       "      <td>11</td>\n",
       "      <td>11</td>\n",
       "      <td>NaN</td>\n",
       "      <td>111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Jason Miller</th>\n",
       "      <td>11000000000000000011</td>\n",
       "      <td>1100000000011</td>\n",
       "      <td>110110000010</td>\n",
       "      <td>1100000000010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Jessica Rodriguez</th>\n",
       "      <td>111</td>\n",
       "      <td>11111</td>\n",
       "      <td>11</td>\n",
       "      <td>110001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Jessica Thompson</th>\n",
       "      <td>11111</td>\n",
       "      <td>111</td>\n",
       "      <td>NaN</td>\n",
       "      <td>111100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Laura Mitchell</th>\n",
       "      <td>11</td>\n",
       "      <td>11</td>\n",
       "      <td>11</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Lisa Collins</th>\n",
       "      <td>NaN</td>\n",
       "      <td>11</td>\n",
       "      <td>1111</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Michael Harris</th>\n",
       "      <td>11001</td>\n",
       "      <td>11101</td>\n",
       "      <td>11</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Michael Robertson</th>\n",
       "      <td>11000000000000000</td>\n",
       "      <td>11000</td>\n",
       "      <td>11</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Nathan Torres</th>\n",
       "      <td>11011</td>\n",
       "      <td>111</td>\n",
       "      <td>NaN</td>\n",
       "      <td>110010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Rachel Kim</th>\n",
       "      <td>111101</td>\n",
       "      <td>110000011010</td>\n",
       "      <td>11</td>\n",
       "      <td>111111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Rahul Desai</th>\n",
       "      <td>11</td>\n",
       "      <td>1100000</td>\n",
       "      <td>11</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Sam Jenkins</th>\n",
       "      <td>11</td>\n",
       "      <td>1100111</td>\n",
       "      <td>11</td>\n",
       "      <td>111111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Sam Robertson</th>\n",
       "      <td>111</td>\n",
       "      <td>11</td>\n",
       "      <td>11</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Zachary Patel</th>\n",
       "      <td>111</td>\n",
       "      <td>110</td>\n",
       "      <td>11011</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "episode                               0              1             2              3\n",
       "source_user                                                                        \n",
       "Alex Thompson                       110            110           NaN             11\n",
       "Bill Fredrickson                    NaN            NaN           NaN            111\n",
       "Bradley Carter                      NaN            NaN           NaN         110010\n",
       "Carlos Mendes                     11000             11    1100000001            NaN\n",
       "Chris Anderson                  1111111  1101111111011       1110101          11001\n",
       "Emily Chen                           11             11            11             11\n",
       "Emily Thompson                       11             11           NaN            111\n",
       "Jason Miller       11000000000000000011  1100000000011  110110000010  1100000000010\n",
       "Jessica Rodriguez                   111          11111            11         110001\n",
       "Jessica Thompson                  11111            111           NaN         111100\n",
       "Laura Mitchell                       11             11            11             11\n",
       "Lisa Collins                        NaN             11          1111             11\n",
       "Michael Harris                    11001          11101            11            NaN\n",
       "Michael Robertson     11000000000000000          11000            11             11\n",
       "Nathan Torres                     11011            111           NaN         110010\n",
       "Rachel Kim                       111101   110000011010            11         111111\n",
       "Rahul Desai                          11        1100000            11            NaN\n",
       "Sam Jenkins                          11        1100111            11         111111\n",
       "Sam Robertson                       111             11            11             11\n",
       "Zachary Patel                       111            110         11011             11"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df.label=='inner_actions'].groupby(['source_user', 'episode'])['data'].apply(lambda x: ''.join([('0' if act[:7]=='FAILED ' else '1') for act in x ])).unstack()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6436ac30",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as pl\n",
    "\n",
    "#copy from voter model file\n",
    "action_probabilities = {\n",
    "    # High frequency actions\n",
    "    \"like_toot\": 0.35,  # Most common action\n",
    "    \"boost_toot\": 0.15,  # Common but less than likes\n",
    "    \"post\": 0.20,  # Regular posting\n",
    "    \"reply\": 0.15,\n",
    "    # Medium frequency actions\n",
    "    \"follow\": 0.15,  # Following new accounts\n",
    "    \"unfollow\": 0.00,  # 25,  # Unfollowing accounts\n",
    "    \"print_timeline\": 0.0,  # Reading timeline\n",
    "    # Low frequency actions\n",
    "    \"block_user\": 0.0,  # Blocking problematic users\n",
    "    \"unblock_user\": 0.0,  # Unblocking users\n",
    "    \"delete_posts\": 0.0,  # Deleting own posts\n",
    "    \"update_bio\": 0.0,  # Updating profile\n",
    "    \"print_notifications\": 0.00,  # 25,  # Checking notifications\n",
    "}\n",
    "\n",
    "fig, ax = pl.subplots()\n",
    "dft = df.loc[\n",
    "    (df.event_type == \"action\") & (df.episode > -1) & ~(df.source_user.isin(names_of_focalplayers)),\n",
    "    :,\n",
    "].copy()\n",
    "dft[\"suggested_action\"] = dft[\"data\"].apply(\n",
    "    lambda x: \"post\" if x[\"suggested_action\"] == \"toot\" else x[\"suggested_action\"]\n",
    ")\n",
    "sns.histplot(\n",
    "    data=dft.loc[:, [\"label\", \"suggested_action\"]].melt(),\n",
    "    ax=ax,\n",
    "    x=\"value\",\n",
    "    hue=\"variable\",\n",
    "    multiple=\"dodge\",\n",
    "    shrink=0.75,\n",
    "    bins=20,\n",
    ")\n",
    "dft\n",
    "action_list = [obj.get_text() for obj in ax.get_xticklabels()]\n",
    "ax.plot(\n",
    "    range(len(action_list)),\n",
    "    [action_probabilities[action] * len(dft) for action in action_list],\n",
    "    \".-\",\n",
    ")\n",
    "ax.set_title(\"line is expected count from suggested freqs\")\n",
    "fig, ax = pl.subplots()\n",
    "contingency = pd.crosstab(dft[\"suggested_action\"], df[\"label\"])\n",
    "sns.heatmap(\n",
    "    contingency,\n",
    "    ax=ax,\n",
    "    annot=True,  # Show numbers in cells\n",
    "    fmt=\"d\",  # Format as integers\n",
    "    cmap=\"YlOrRd\",  # Yellow-Orange-Red color scheme\n",
    "    cbar_kws={\"label\": \"Count\"},\n",
    "    square=True,\n",
    ")  # Make cells square\n",
    "ax.set_ylabel(\"action taken\")\n",
    "ax.set_ylabel(\"suggested action\")\n",
    "ax.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df00ca04",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
